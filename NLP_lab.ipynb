{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWeriYkusO7vo5cUyxmuLx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11abdellah11/Spam_classification/blob/main/NLP_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qZVrRMIZ1l4i"
      },
      "outputs": [],
      "source": [
        "paragraphe = \"Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1. Its fine-tuned models have been trained on over 1 million human annotations.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "cQBoikMO2zMS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuse73Hs3Q-H",
        "outputId": "5a08123e-ee1c-4fde-8800-70a28f48f554"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.sent_tokenize(paragraphe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwty5SrU20sN",
        "outputId": "d61e532d-f07e-4be6-f69a-149642994b31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.',\n",
              " 'Its fine-tuned models have been trained on over 1 million human annotations.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(paragraphe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuVJwBbH3IBf",
        "outputId": "51b95922-63f4-43a6-c12a-23e2586f594e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Llama',\n",
              " '2',\n",
              " 'pretrained',\n",
              " 'models',\n",
              " 'are',\n",
              " 'trained',\n",
              " 'on',\n",
              " '2',\n",
              " 'trillion',\n",
              " 'tokens',\n",
              " ',',\n",
              " 'and',\n",
              " 'have',\n",
              " 'double',\n",
              " 'the',\n",
              " 'context',\n",
              " 'length',\n",
              " 'than',\n",
              " 'Llama',\n",
              " '1',\n",
              " '.',\n",
              " 'Its',\n",
              " 'fine-tuned',\n",
              " 'models',\n",
              " 'have',\n",
              " 'been',\n",
              " 'trained',\n",
              " 'on',\n",
              " 'over',\n",
              " '1',\n",
              " 'million',\n",
              " 'human',\n",
              " 'annotations',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiHbQUlI3jnb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yuRzTbblhj6",
        "outputId": "a40379c1-435d-4b8a-c57b-951b9445b0e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "lIQq7RY2lhgm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "tmUWeEQblhdr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"million\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gs_WE6QHlhbV",
        "outputId": "75a569ae-c832-4c1a-83f9-8aebc5d1a756"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'million'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "tMhrPr8rlhZE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "zZ-CpaxDlhV6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem(\"eating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fwVkXQqpm4zV",
        "outputId": "926082d1-824a-411c-a125-4776eba1961d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraphe)"
      ],
      "metadata": {
        "id": "E1Nr8fxon5R8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words]\n",
        "  corpus.append(\" \".join(words))\n",
        "  #print (words)"
      ],
      "metadata": {
        "id": "SxU4EvO_m9Bm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qEgkCup1qvup",
        "outputId": "4db2ae2c-a625-4756-af09-865b7aba5b58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it fine-tun model have been train on over 1 million human annot .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_cJZL2Sryed",
        "outputId": "36c47180-38b0-4814-a209-d0b2699bff88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['llama 2 pretrain model are train on 2 trillion token , and have doubl the context length than llama 1 .',\n",
              " 'it fine-tun model have been train on over 1 million human annot .']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asHDihkqr48X",
        "outputId": "1499da90-9901-45a5-e2c1-d0ddb56ef13b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "9PhIspyl6QPo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")# all the stop words"
      ],
      "metadata": {
        "id": "wF9VZX_J6ZKf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words(\"french\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WciSIhq767jY",
        "outputId": "229b6c6c-f862-4623-a0ea-e6e5c69c4887"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['au',\n",
              " 'aux',\n",
              " 'avec',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'et',\n",
              " 'eux',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'je',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'même',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'ses',\n",
              " 'son',\n",
              " 'sur',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'c',\n",
              " 'd',\n",
              " 'j',\n",
              " 'l',\n",
              " 'à',\n",
              " 'm',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'y',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étants',\n",
              " 'étantes',\n",
              " 'suis',\n",
              " 'es',\n",
              " 'est',\n",
              " 'sommes',\n",
              " 'êtes',\n",
              " 'sont',\n",
              " 'serai',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'serons',\n",
              " 'serez',\n",
              " 'seront',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'serions',\n",
              " 'seriez',\n",
              " 'seraient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étions',\n",
              " 'étiez',\n",
              " 'étaient',\n",
              " 'fus',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fûtes',\n",
              " 'furent',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'soyons',\n",
              " 'soyez',\n",
              " 'soient',\n",
              " 'fusse',\n",
              " 'fusses',\n",
              " 'fût',\n",
              " 'fussions',\n",
              " 'fussiez',\n",
              " 'fussent',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eus',\n",
              " 'ai',\n",
              " 'as',\n",
              " 'avons',\n",
              " 'avez',\n",
              " 'ont',\n",
              " 'aurai',\n",
              " 'auras',\n",
              " 'aura',\n",
              " 'aurons',\n",
              " 'aurez',\n",
              " 'auront',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'aurions',\n",
              " 'auriez',\n",
              " 'auraient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avions',\n",
              " 'aviez',\n",
              " 'avaient',\n",
              " 'eut',\n",
              " 'eûmes',\n",
              " 'eûtes',\n",
              " 'eurent',\n",
              " 'aie',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'ayons',\n",
              " 'ayez',\n",
              " 'aient',\n",
              " 'eusse',\n",
              " 'eusses',\n",
              " 'eût',\n",
              " 'eussions',\n",
              " 'eussiez',\n",
              " 'eussent']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create our list of stop words"
      ],
      "metadata": {
        "id": "xb8X1HjE6cUE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [\"changes\",\"change\",\"changing\",\"changer\",\"changeable\"]:\n",
        "    print(lemmatizer.lemmatize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA2P95Cu8R27",
        "outputId": "51ee2db9-f522-499d-c9e5-b0f807c9251d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change\n",
            "change\n",
            "changing\n",
            "changer\n",
            "changeable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FBfbuiX8SQ3",
        "outputId": "e829c699-d751-4876-ae24-8ed066fc3fbf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['llama 2 pretrain model are train on 2 trillion token , and have doubl the context length than llama 1 .',\n",
              " 'it fine-tun model have been train on over 1 million human annot .']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords from corpus\n",
        "corpus=[]\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i].lower())\n",
        "    words=[stemmer.stem(word) for word in words if not word in stop_words]\n",
        "    print(words)\n",
        "    corpus.append(' '.join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXy05kjZ8fbx",
        "outputId": "7f7577fd-4fbc-4ff2-8913-7c6b221dce97"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['llama', '2', 'pretrain', 'model', 'train', '2', 'trillion', 'token', ',', 'doubl', 'context', 'length', 'llama', '1', '.']\n",
            "['fine-tun', 'model', 'train', '1', 'million', 'human', 'annot', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y887xtx884Tx",
        "outputId": "3dc41d22-9660-448c-cd61-912b791d265d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['llama 2 pretrain model train 2 trillion token , doubl context length llama 1 .',\n",
              " 'fine-tun model train 1 million human annot .']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "Xw3N9Hxh86Af"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh2kJmmQ942r",
        "outputId": "a1078cda-a0ab-492a-b448-ff6b6c3d466c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.',\n",
              " 'Its fine-tuned models have been trained on over 1 million human annotations.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "corpus=[]\n",
        "for i in range(len(sentences)):\n",
        "    text=re.sub('[^a-zA-Z0-9]',' ',sentences[i]) #\n",
        "    text=text.lower()\n",
        "    words=text.split()\n",
        "    words=[stemmer.stem(word) for word in words if not word in stop_words]\n",
        "    print(words)\n",
        "    corpus.append(' '.join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAwtwt07dcyF",
        "outputId": "d79685be-3be3-4c4a-8f96-f3071fd4fb04"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['llama', '2', 'pretrain', 'model', 'train', '2', 'trillion', 'token', 'doubl', 'context', 'length', 'llama', '1']\n",
            "['fine', 'tune', 'model', 'train', '1', 'million', 'human', 'annot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cv=CountVectorizer(binary=True,ngram_range=(1,2))"
      ],
      "metadata": {
        "id": "M0tOhRrddi8q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##applied BOW / bag of words\n",
        "# \"The Bag-of-Words (BoW) model is one of the most fundamental and widely used techniques in Natural Language Processing (NLP).\n",
        "#The model is used to represent\n",
        "#text data as a collection of its constituent words, or tokens, disregarding grammar and word order\"\n"
      ],
      "metadata": {
        "id": "3dHMS9Z0q6p3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE6-5iXLrXp2",
        "outputId": "2ec68669-9457-420c-a833-27abc2b21bed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        0, 1, 1, 1, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
              "        1, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(open('tfidf.pkl','wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "QupP4rlfraYi",
        "outputId": "74287ff9-da0f-43a2-90b0-ec89991d2f92"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "dump() missing required argument 'file' (pos 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ed5d46c024f2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: dump() missing required argument 'file' (pos 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdJ5c1KIrk4T",
        "outputId": "232855e1-58b5-47c1-a2ef-4a6f5217e0f3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'llama': 11,\n",
              " 'pretrain': 17,\n",
              " 'model': 15,\n",
              " 'train': 21,\n",
              " 'trillion': 24,\n",
              " 'token': 19,\n",
              " 'doubl': 3,\n",
              " 'context': 1,\n",
              " 'length': 9,\n",
              " 'llama pretrain': 12,\n",
              " 'pretrain model': 18,\n",
              " 'model train': 16,\n",
              " 'train trillion': 23,\n",
              " 'trillion token': 25,\n",
              " 'token doubl': 20,\n",
              " 'doubl context': 4,\n",
              " 'context length': 2,\n",
              " 'length llama': 10,\n",
              " 'fine': 5,\n",
              " 'tune': 26,\n",
              " 'million': 13,\n",
              " 'human': 7,\n",
              " 'annot': 0,\n",
              " 'fine tune': 6,\n",
              " 'tune model': 27,\n",
              " 'train million': 22,\n",
              " 'million human': 14,\n",
              " 'human annot': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "xrIeKUlxroc5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf=TfidfVectorizer()"
      ],
      "metadata": {
        "id": "tGa-p6JKrrsm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9indZEaFr-h9",
        "outputId": "f2974e45-db88-4823-9cbf-878f000a6d4b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.30134034, 0.30134034, 0.        , 0.        ,\n",
              "        0.30134034, 0.60268068, 0.        , 0.21440614, 0.30134034,\n",
              "        0.30134034, 0.21440614, 0.30134034, 0.        ],\n",
              "       [0.4078241 , 0.        , 0.        , 0.4078241 , 0.4078241 ,\n",
              "        0.        , 0.        , 0.4078241 , 0.29017021, 0.        ,\n",
              "        0.        , 0.29017021, 0.        , 0.4078241 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKKmmwIQsEOf",
        "outputId": "70c7d338-7f9a-4a9e-91d1-c29be6a071c4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'llama': 6,\n",
              " 'pretrain': 9,\n",
              " 'model': 8,\n",
              " 'train': 11,\n",
              " 'trillion': 12,\n",
              " 'token': 10,\n",
              " 'doubl': 2,\n",
              " 'context': 1,\n",
              " 'length': 5,\n",
              " 'fine': 3,\n",
              " 'tune': 13,\n",
              " 'million': 7,\n",
              " 'human': 4,\n",
              " 'annot': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0B4uxkcsGWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}